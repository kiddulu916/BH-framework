---
description: Used when referred by @@EPIC or @@EPICS or when writing or completing any epic level of work or epic level of capabilities.
globs: 
alwaysApply: true
---

# **Epic Planning & Tracking System**

**Purpose:**
Plan and track all major initiatives for the Bug Hunting Framework using a unified, architecture-driven approach. All epics are mapped to the domains and flows defined in `architecture.mdc`.

**Hierarchy:**
- **EPIC GROUP**: Optional grouping of related epics
- **EPIC**: Major business or technical initiative
- **PHASE**: Milestone within an epic
- **STEP**: Actionable feature or deliverable

---

## EPIC PORTFOLIO STATUS

- **Total Active Epics**: 3 (Max: 3)
- **Completed Epics**: 1
- **In Progress Epics**: 1
- **Blocked Epics**: 0
- **Last Updated**: 2025-01-27

**Legend:**
- ‚úÖ COMPLETED
- üîÑ IN_PROGRESS
- ‚è≥ PLANNED
- üö´ BLOCKED
- ‚è∏Ô∏è PAUSED
- ‚ùå CANCELLED
- üì¶ ARCHIVED

---

## ACTIVE EPICS

### 1. **EPIC**: PLATFORM_INFRASTRUCTURE_EPIC

**Status:** COMPLETED
**Priority:** HIGH
**Started:** 2025-06-07
**Target Completion:** 2025-08-01

#### **Goal**

Establish the core infrastructure containers (`db`, `backend`, `frontend`) and the communication layer that enables the stage-based workflow.

#### **Success Criteria**

- [ ] Docker Compose orchestrates `db`, `backend`, and `frontend` containers successfully with persistent data and inter-container networking.
- [ ] Backend API provides fully tested and documented endpoints for stage management and result submission, including validation schemas.
- [ ] Database schema, created via Alembic migrations, is in place to store results from all stages with appropriate relations.
- [ ] Secure, environment-aware communication is established between all infrastructure containers.

#### Dependencies & Blockers

- None (foundation epic)

#### **PHASE 1:** CORE_INFRASTRUCTURE_SETUP_PHASE - ‚úÖ COMPLETED

**Goal:** Stand up, configure, and network the three main infrastructure containers.

**STEPS:**

1. **DB_CONTAINER_STEP**: Implement the PostgreSQL `db` container.
    - **SUB-STEP**: Define the `postgres` service in `docker-compose.yml`.
    - **Sub-step**: Configure environment variables for the database user, password, and name.
    - **Sub-step**: Mount a local volume (`./data/postgres`) to `/var/lib/postgresql/data` for data persistence.
    - **Sub-step**: Expose the port (e.g., `5432:5432`) for local debugging access. - ‚úÖ 100%

2. **BACKEND_CONTAINER_STEP**: Implement the Django Ninja `backend` container.
    - **Sub-step**: Create a `Dockerfile` specifying the Python version, installing dependencies from `requirements.txt`, and setting the `CMD` to run the ASGI server (e.g., Gunicorn with Uvicorn workers).
    - **Sub-step**: Configure the `backend` service in `docker-compose.yml` to build from the Dockerfile.
    - **Sub-step**: Set `depends_on: [db]` to ensure correct startup order.
    - **Sub-step**: Pass the database connection URL via an environment variable. - ‚úÖ 100%

3. **FRONTEND_CONTAINER_STEP**: Implement the Next.js `frontend` container.
    - **Sub-step**: Create a `Dockerfile` to install Node.js dependencies (`pnpm install`) and start the Next.js development server.
    - **Sub-step**: Configure the `frontend` service in `docker-compose.yml`.
    - **Sub-step**: Configure an environment variable for the backend API URL (`http://backend:8000`). - ‚úÖ 100%

4. **DOCKER_COMPOSE_STEP**: Finalize the orchestration and networking.
    - **Sub-step**: Create a shared bridge network in `docker-compose.yml` to allow containers to communicate via service names.
    - **Sub-step**: Write startup scripts or health checks to ensure services are fully operational before dependent services start.
    - **Sub-step**: Document all environment variables in a `.env` file. - ‚úÖ 100%

**Completion Date**: 2025-06-21
**Key Outcomes**: 
- All three infrastructure containers (db, backend, frontend) successfully orchestrated
- Health checks implemented and passing for all services
- Development environment with hot reloading configured
- Comprehensive documentation and setup instructions created
- Database connection and inter-container communication working

#### **PHASE 2:** STAGE_API_INTEGRATION_PHASE - ‚úÖ COMPLETED

**Goal:** Build and test the backend API endpoints and database schemas required to manage and integrate the Stage Containers.

**STEPS:**

1. **DB_SCHEMA_STEP**: Design and implement the database schema using SQLAlchemy and Alembic.
    - **Sub-step**: Create base models in SQLAlchemy with shared fields (`id`, `created_at`, `updated_at`).
    - **Sub-step**: Define models for each stage's results (`PassiveReconResult`, `ActiveReconResult`, `Vulnerability`, etc.) with appropriate fields and relationships (e.g., foreign key to a `Target` table).
    - **Sub-step**: Generate an initial Alembic migration script and apply it to the database.
    - **Sub-step**: Implement repository classes for each model to abstract database queries. - ‚úÖ 100%
2.  **TOOL_RESULT_API_STEP**: Create backend endpoints for Stage Containers to submit their tool outputs.
    - **Sub-step**: Define Pydantic schemas for result submission to ensure data validation.
    - **Sub-step**: Implement the API endpoint (e.g., `POST /api/v1/targets/{target_id}/results/{stage_name}`) in a Django Ninja router.
    - **Sub-step**: Write the service logic to process the validated data and use the appropriate repository to save it to the database.
    - **Sub-step**: Implement unit and integration tests for this endpoint. - ‚úÖ 100%
3.  **STAGE_EXECUTION_API_STEP**: Create backend endpoints for the frontend to trigger a stage for a target.
    - **Sub-step**: Define a Pydantic schema for the stage execution request.
    - **Sub-step**: Implement the API endpoint (e.g., `POST /api/stages/{stage_name}/execute`).
    - **Sub-step**: Write the backend logic to receive this request and dispatch a job to run the corresponding Stage Container (this may involve a job queue like Celery or a direct Docker command, to be decided).
    - **Sub-step**: Implement tests to verify that a valid request triggers the stage execution logic. - ‚úÖ 100%

**Completion Date**: 2025-01-27
**Key Outcomes**: 
- Comprehensive database schema implemented with all stage result models
- Complete API endpoints for result submission and stage execution
- Full integration test suite passing with proper JWT authentication
- URL routing patterns established and documented
- Enum handling and schema validation patterns implemented
- Error handling and validation standards established

---

### 2. **EPIC:** BUG_HUNTING_STAGES_EPIC
**Status:** IN_PROGRESS
**Priority:** HIGH
**Started:** 2025-06-07
**Target Completion:** 2025-09-01

#### **Goal**
Build, tool, and integrate each specialized Stage Container, ensuring they can execute their tasks, communicate with the backend, and use data from previous stages.

#### **Success Criteria**
- [ ] Each Stage Container has a documented, reproducible build via its `Dockerfile`.
- [ ] Each Stage Container includes a robust execution script that handles target input, tool execution, output parsing, and API submission.
- [ ] Data from earlier stages is successfully queried and utilized by later stages.
- [ ] The entire chain from passive recon to reporting is functional and tested end-to-end.

#### Dependencies & Blockers
- `PLATFORM_INFRASTRUCTURE_EPIC` must be completed first.

#### **PHASE 1:** RECONNAISSANCE_STAGES_PHASE - üîÑ IN_PROGRESS
**Goal:** Implement the initial information-gathering stages, both passive and active.

**Steps:**
1.  **PASSIVE_RECON_CONTAINER_STEP**: Build the `passive_recon` container.
    - **Sub-step**: Create a `Dockerfile` with a lightweight base image (e.g., `python:3.11-slim`) and install tools like `subfinder`, `amass`, and `assetfinder`.
    - **Sub-step**: Write a Python execution script (`run.py`) that accepts a target domain, runs the tools, normalizes their outputs into a consistent JSON format, and handles errors.
    - **Sub-step**: In the script, implement the logic to `POST` the JSON results to the backend's `TOOL_RESULT_API_STEP` endpoint.
    - **Sub-step**: Add this container as a service definition in the main `docker-compose.yml`. - ‚úÖ 100%
2.  **ACTIVE_RECON_CONTAINER_STEP**: Build the `active_recon` container.
    - **Sub-step**: Create a `Dockerfile` and install tools like `nmap` and `httpx`.
    - **Sub-step**: Write an execution script that first queries the backend API to fetch the subdomains discovered in the passive stage.
    - **Sub-step**: The script then runs `nmap` and `httpx` on those subdomains to identify open ports, services, and live web servers.
    - **Sub-step**: It then parses these results and posts them to the backend API. - ‚è≥ 0%

#### **PHASE 2:** VULNERABILITY_ANALYSIS_STAGES_PHASE - ‚è≥ PLANNED
**Goal:** Implement the stages for automated vulnerability scanning and manual verification.

**Steps:**
1.  **VULN_SCANNING_CONTAINER_STEP**: Build the `vulnerability_scanning` container.
    - **Sub-step**: Create a `Dockerfile` and install tools like `nuclei` and a curated set of templates.
    - **Sub-step**: The execution script will query the database for live web servers identified in the active recon stage.
    - **Sub-step**: It will then run `nuclei` against these targets and post any findings (categorized by severity) to the backend. - ‚è≥ 0%
    - **Sub-step**: Then it will lookup the found ports/services found running for each IP/subdomain and run the relevant tools used to assess those services for vulnerabilities and post any findings (categorized by severity) to the backend. - ‚è≥ 0%
2.  **VULN_TESTING_CONTAINER_STEP**: Build the `vulnerability_testing` container.
    - **Sub-step**: Create a `Dockerfile` and install interactive or verification-focused tools like `sqlmap`, `ffuf`, etc.
    - **Sub-step**: This stage will gather the findings found in last stage and then execute scripts/tool to test any potential vulnerability, (e.g., a URL potentially vulnerable to SQLi) ethically using industries best-practice.
    - **Sub-step**: The script will post the verified finding, including proof-of-concept data, to the backend. - ‚è≥ 0%

#### **PHASE 3:** ANALYSIS_AND_REPORTING_STAGES_PHASE - ‚è≥ PLANNED
**Goal:** Implement the final stages for data aggregation, analysis, and report generation.

**Steps:**
1.  **KILL_CHAIN_ANALYSIS_CONTAINER_STEP**: Build the `kill_chain_analysis` container.
    - **Sub-step**: This container might not have many external tools but will contain complex analysis scripts.
    - **Sub-step**: The script will query the backend for all data related to a target (subdomains, services, vulnerabilities, ports).
    - **Sub-step**: It will then apply a set of rules to identify potential attack paths (e.g., "Public S3 bucket" + "Exposed API key in code" = "Data Exfiltration Path"). The identified chains are saved back to the database. 
    - **SUB-STEP**: Then it will attempt to verify any potential kill-chain-path while still maintaining industry best-practices and ethicacies and take screenshots for every step taken for the reporting stage and to put together a proof-of-concept. - ‚è≥ 0%
2.  **REPORT_GENERATION_CONTAINER_STEP**: Build the `report_generation` container.
    - **Sub-step**: Create a `Dockerfile` and install libraries for PDF or Markdown generation (e.g., `pandoc`).
    - **Sub-step**: The script will query all verified findings and analysis from the database.
    - **Sub-step**: It will populate a predefined report template with this data and generate the final report file.
    - **Sub-step**: The generated report will be stored in a shared volume or uploaded to a location accessible by the backend/frontend. - ‚è≥ 0%

---

### 3. **EPIC:** USER_EXPERIENCE_EPIC
**Status:** PLANNED
**Priority:** MEDIUM
**Started:** 2025-06-07
**Target Completion:** 2025-10-01

#### **Goal**
Deliver a modern, intuitive, and responsive frontend that allows users to seamlessly initiate, monitor, and review results from each stage of the bug hunting workflow.

#### **Success Criteria**
- [ ] A clear, responsive UI allows users to manage targets (add, edit, delete) and start workflows.
- [ ] Each stage has a dedicated, well-designed view to display its specific results in a filterable and sortable format.
- [ ] A unified dashboard provides a real-time, at-a-glance overview of a hunt's progress across all stages.
- [ ] Users can generate, view, download, and export final reports and raw data.
- [ ] The frontend provides robust user authentication and profile management.

#### Dependencies & Blockers
- `PLATFORM_INFRASTRUCTURE_EPIC` and `BUG_HUNTING_STAGES_EPIC` must be completed first.

#### **PHASE 1:** CORE_UI_&_AUTH_PHASE - ‚è≥ PLANNED
**Goal:** Build the foundational UI components (No user registration or authentication) Only record users name for the reporting stage and for preserving the entire session per user.

**Steps:**
1.  **DASHBOARD_UI_STEP**: Create the main dashboard.
    - **Sub-step**: Design the layout for displaying a list of targets and their high-level status.
    - **SUB_STEP**: Design and layout a reusable page template for target profiles that display all data found on that target if any at all. Target profile 
    - **Sub-step**: Implement API calls(CRUD) for target data from the backend.
    - **Sub-step**: Create reusable React components like `TargetCard` and `StatusBadge`. - ‚è≥ 0%
2.  **TARGET_MANAGEMENT_UI_STEP**: Build UI for adding, editing, and removing targets.
    - **Sub-step**: Create a modal form for adding/editing targets with input validation.
    - **Sub-step**: Implement the API logic to `POST`, `PUT`, and `DELETE` targets on the backend. - ‚è≥ 0%
3.  **USER_REGISTRATION_UI_STEP**: Implement frontend registration, and profile management.
    - **Sub-step**: Create registration pages/forms.
    - **Sub-step**: Integrate with the backend's JWT authorization tokens to validate the user to operate within the created profile for that user.
    - **Sub-step**: Implement protected routes that require a user to have correct JWT token for api calls and database access per users profiles.
    - **Sub-step**: Create a user profile page to manage account details. Mainly for use of report generations - ‚è≥ 0%

#### **PHASE 2:** STAGE_VIEW_UI_PHASE - ‚è≥ PLANNED
**Goal:** Create dedicated, interactive UI views for each bug hunting stage.

**Steps:**
1.  **RECON_VIEW_UI_STEP**: UI to display results from passive and active recon.
    - **Sub-step**: Design a view with tables to display discovered subdomains, open ports, and technologies.
    - **Sub-step**: Implement filtering and sorting for the data tables.
    - **Sub-step**: Add a "Start Vulnerability Scan" button that calls the `STAGE_EXECUTION_API_STEP` on the backend. - ‚è≥ 0%
2.  **VULNERABILITY_VIEW_UI_STEP**: UI to display findings from the scanning and testing stages.
    - **Sub-step**: Design a view to list vulnerabilities, including severity, description, and status (e.g., "Verified", "False Positive").
    - **Sub-step**: Implement functionality for the user to triage findings and manually trigger the `Vulnerability Testing` stage for a specific item. - ‚è≥ 0%
3.  **ANALYSIS_VIEW_UI_STEP**: UI to visualize the kill-chain analysis.
    - **Sub-step**: Design a view to display the identified attack paths. This could be a list, a tree, or a graph visualization (e.g., using a library like `react-flow`).
    - **Sub-step**: Allow users to click on parts of the chain to see the underlying evidence.
    - **Sub-step**: Provide an interface to display any verified POCs and/or create and document custom POCs. - ‚è≥ 0%
4.  **REPORTING_VIEW_UI_STEP**: UI to generate, customize, and download reports and data.
    - **Sub-step**: Create a view with a button to trigger the `Report Generation` stage.
    - **Sub-step**: Display the status of the report generation.
    - **Sub-step**: Provide a download link for the completed report.
    - **Sub-step**: Add options to export raw data from any stage as CSV or JSON. - ‚è≥ 0%

---

## EPIC COMPLETION HISTORY

*No completed epics yet. Completed epics will be listed here with completion date and key outcomes.*

---

## EPIC REPORTING AND RETROSPECTIVES

### Epic Retrospective Template
```
## Epic Retrospective: [Epic Name]

### Epic Summary
- **Final Status**: [Completed/Cancelled with reason]
- **Key Phases Completed**: [List major phases finished]
- **Original vs Actual Scope**: [Any scope changes made]

### What Went Well
- [Positive aspects of the epic execution]

### What Could Be Improved
- [Areas for improvement in future epics]

### Lessons Learned
- [Key learnings to apply to future epics]

### Recommendations
- [Specific recommendations for future epic planning/execution]
```

---

## EPIC MANAGEMENT GUIDELINES

- All epics must be mapped to domains and flows in `architecture.mdc`
- Maximum 3 active epics for context effectiveness
- Use workflow-state.mdc for step-by-step execution and progress tracking
- Update this file as new epics are planned or completed
- Archive completed epics older t
